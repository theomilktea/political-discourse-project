{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38660622-edfe-4b10-8d4c-db22bdcb5c64",
   "metadata": {},
   "source": [
    "Political Discourse Keyword Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d348871-0890-408f-a29e-12e2d167d623",
   "metadata": {},
   "source": [
    "This notebook will explore word usage patterns across 20 speeches made by current Secretary General AntÃ³nio Guterres using basic NLP and text analysis techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da05b93d-6e00-4362-a939-dec2f4eb5d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c88f3c0-98bb-47af-a4fd-a7d5b067499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/raw/\"\n",
    "files = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b9e3a5-2fa6-4f11-8b2d-91b6cb8a6240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 speeches\n"
     ]
    }
   ],
   "source": [
    "speeches = []\n",
    "for file in sorted(os.listdir(data_path)):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(data_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            speeches.append(text)\n",
    "\n",
    "print(f\"Loaded {len(speeches)} speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "589ba346-32b0-43df-9a28-dc7e31a19e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As we enter the new year, the world stands at a crossroads.\n",
      "\n",
      "\n",
      "Chaos and uncertainty surround us. \n",
      "\n",
      "\n",
      "Division. Violence. Climate breakdown. And systemic violations of international law.\n",
      "\n",
      "\n",
      "A retreat from the very principles that bind us together as a human family. \n",
      "\n",
      "\n",
      "People everywhere are asking: Are leaders even listening? Are they ready to act.\n",
      "\n",
      "\n",
      "As we turn the page on a turbulent year, one fact s\n"
     ]
    }
   ],
   "source": [
    "print(speeches[0][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b824ec02-3530-4b7c-aeb9-8da75e8dbb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abb00cbb-7b6c-40a1-b873-cf1a789cc8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as we enter the new year the world stands at a crossroads\n",
      "\n",
      "\n",
      "chaos and uncertainty surround us \n",
      "\n",
      "\n",
      "division violence climate breakdown and systemic violations of international law\n",
      "\n",
      "\n",
      "a retreat from the very principles that bind us together as a human family \n",
      "\n",
      "\n",
      "people everywhere are asking are leaders e\n"
     ]
    }
   ],
   "source": [
    "cleaned_speeches = []\n",
    "for speech in speeches:\n",
    "    text = speech.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    cleaned_speeches.append(text)\n",
    "\n",
    "print(cleaned_speeches[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5324499-79cd-4922-a401-8dc9486c4dc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_speeches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m      2\u001b[39m nlp = spacy.load(\u001b[33m\"\u001b[39m\u001b[33men_core_web_sm\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m doc = nlp(\u001b[43mcleaned_speeches\u001b[49m[\u001b[32m0\u001b[39m])\n\u001b[32m      5\u001b[39m tokens = [token.text \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m token.is_alpha]\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokens[:\u001b[32m30\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'cleaned_speeches' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(cleaned_speeches[0])\n",
    "tokens = [token.text for token in doc if token.is_alpha]\n",
    "\n",
    "print(tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684925af-a861-415a-8bbe-68e8edc1dcb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
