{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38660622-edfe-4b10-8d4c-db22bdcb5c64",
   "metadata": {},
   "source": [
    "Political Discourse Keyword Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d348871-0890-408f-a29e-12e2d167d623",
   "metadata": {},
   "source": [
    "This notebook will explore word usage patterns across 20 speeches made by current Secretary General Ant√≥nio Guterres using basic NLP and text analysis techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da05b93d-6e00-4362-a939-dec2f4eb5d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard library\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "#Importing spaCy for NLP\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a7825e-73ff-47dd-a6ac-b79839d756da",
   "metadata": {},
   "source": [
    "Loading in the speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c88f3c0-98bb-47af-a4fd-a7d5b067499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 speeches\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/raw/\"\n",
    "\n",
    "speeches = []\n",
    "for file in sorted(os.listdir(data_path)):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(data_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            speeches.append(f.read())\n",
    "\n",
    "print(f\"Loaded {len(speeches)} speeches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74fcef-1f1e-45c9-a6a1-bd5759f190df",
   "metadata": {},
   "source": [
    "Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb00cbb-7b6c-40a1-b873-cf1a789cc8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as we enter the new year the world stands at a crossroads\n",
      "\n",
      "\n",
      "chaos and uncertainty surround us \n",
      "\n",
      "\n",
      "division violence climate breakdown and systemic violations of international law\n",
      "\n",
      "\n",
      "a retreat from the very principles that bind us together as a human family \n",
      "\n",
      "\n",
      "people everywhere are asking are leaders e\n"
     ]
    }
   ],
   "source": [
    "cleaned_speeches = []\n",
    "\n",
    "for speech in speeches:\n",
    "    text = speech.lower() #Making each word lowercase\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text) #Removing all characters except lowercase letters and whitespace\n",
    "    cleaned_speeches.append(text)\n",
    "\n",
    "#Previewing first cleaned speeches\n",
    "print(cleaned_speeches[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040cda81-fc1c-4150-a21d-067c04f85807",
   "metadata": {},
   "source": [
    "Tokenization + stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5324499-79cd-4922-a401-8dc9486c4dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of content words: 3438\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") #Loading the small English language module pipeline\n",
    "stopwords = nlp.Defaults.stop_words #Default list of all stop words (\"the,\" \"is,\" \"at\")\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "for speech in cleaned_speeches:\n",
    "    doc = nlp(speech) #spaCy tokenizes, tags, and parses the speech\n",
    "    tokens = [\n",
    "        token.text #The string of the word\n",
    "        for token in doc \n",
    "        if token.is_alpha and token.text.lower() not in stopwords #Removes numbers/symbols, excludes stopwords\n",
    "    ]\n",
    "    all_tokens.extend(tokens) #Add filtered words to master list\n",
    "\n",
    "print(f\"Total number of content words: {len(all_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e51438-c924-4e6d-be51-18b7ecb4f8f0",
   "metadata": {},
   "source": [
    "Analyzing word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2866c72c-533a-4d3d-940f-1fe06b7a2d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('people', 31),\n",
       " ('united', 30),\n",
       " ('nations', 30),\n",
       " ('world', 27),\n",
       " ('international', 23),\n",
       " ('peace', 20),\n",
       " ('years', 20),\n",
       " ('year', 18),\n",
       " ('humanitarian', 18),\n",
       " ('human', 17),\n",
       " ('global', 16),\n",
       " ('today', 16),\n",
       " ('thank', 16),\n",
       " ('rights', 16),\n",
       " ('communities', 16),\n",
       " ('excellencies', 15),\n",
       " ('women', 15),\n",
       " ('support', 14),\n",
       " ('help', 14),\n",
       " ('progress', 13)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = Counter(all_tokens) #Counts the frequency of each word\n",
    "word_counts.most_common(20) #Retrieves the top 20 most frequent words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
